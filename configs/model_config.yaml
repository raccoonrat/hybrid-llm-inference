# hybrid-llm-inference/configs/model_config.yaml
models:
  TinyLlama-1.1B-Chat-v1.0:
    model_name: TinyLlama-1.1B-Chat-v1.0
    type: local
    model_path: "D:\\models\\TinyLlama-1.1B-Chat-v1.0"
    mode: local
    tokenizer: auto
    max_length: 2048
    batch_size: 1
    device: cuda
    dtype: float16
    cache_dir: data/models
    trust_remote_code: true
    mixed_precision: fp16
    device_placement: true
    search_range: [16, 100]
