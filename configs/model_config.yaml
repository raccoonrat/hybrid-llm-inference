# hybrid-llm-inference/configs/model_config.yaml
models:
  llama3:
    model_name: meta-llama/Llama-3-8B
    mode: local
    max_length: 512
    mixed_precision: fp16
    device_placement: true
  falcon:
    model_name: tiiuae/falcon-7b
    mode: local
    max_length: 512
    mixed_precision: fp16
    device_placement: true
  mistral:
    model_name: mistralai/Mixtral-7B-v0.1
    mode: local
    max_length: 512
    mixed_precision: fp16
    device_placement: true
  tinyllama:
    model_name: TinyLlama-1.1B-Chat-v1.0
    type: local
    path: "\\\\wsl.localhost\\Ubuntu-24.04\\home\\mpcblock\\models\\TinyLlama-1.1B-Chat-v1.0"
    tokenizer: auto
    max_length: 2048
    batch_size: 1
    device: cuda
    dtype: float16
    precision: float16
    quantization: none
    cache_dir: data/models
    trust_remote_code: true
    mixed_precision: fp16
    device_placement: true
