"""TokenProcessor å’Œ TokenProcessing çš„æ‰©å±•æµ‹è¯•ç”¨ä¾‹ã€‚"""

import os
os.environ['TEST_MODE'] = '1'

import pytest
import pandas as pd
import numpy as np
from pathlib import Path
import json
from src.data_processing.token_processor import TokenProcessor, MockTokenizer
from src.data_processing.token_processing import TokenProcessing
from src.data_processing.data_processor import DataProcessor
from src.data_processing.alpaca_loader import AlpacaLoader
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯
import time
import tempfile

@pytest.fixture
def model_path(tmp_path):
    """åˆ›å»ºæµ‹è¯•ç”¨çš„æ¨¡å‹è·¯å¾„ã€‚"""
    model_dir = tmp_path / "models" / "TinyLlama-1.1B-Chat-v1.0"
    model_dir.mkdir(parents=True)
    return str(model_dir)

@pytest.fixture
def invalid_model_path(tmp_path):
    """åˆ›å»ºæ— æ•ˆçš„æ¨¡å‹è·¯å¾„ã€‚"""
    return str(tmp_path / "nonexistent_model")

@pytest.fixture
def large_text_data():
    """åˆ›å»ºå¤§æ–‡æœ¬æ•°æ®ã€‚"""
    return ["Hello world! " * 1000,  # é•¿æ–‡æœ¬
            "",  # ç©ºæ–‡æœ¬
            "Special chars: !@#$%^&*()",  # ç‰¹æ®Šå­—ç¬¦
            "Numbers: 1234567890",  # æ•°å­—
            "Unicode: ä½ å¥½ä¸–ç•ŒğŸŒ"  # Unicodeå­—ç¬¦
            ]

# TokenProcessor çš„æ‰©å±•æµ‹è¯•

def test_token_processor_invalid_model_path(invalid_model_path):
    """æµ‹è¯•ä½¿ç”¨æ— æ•ˆçš„æ¨¡å‹è·¯å¾„åˆå§‹åŒ– TokenProcessorã€‚"""
    with pytest.raises(ValueError, match="æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨"):
        TokenProcessor(model_path=invalid_model_path, validate_path=True)

def test_token_processor_empty_input(model_path):
    """æµ‹è¯•å¤„ç†ç©ºè¾“å…¥ã€‚"""
    processor = TokenProcessor(model_path=model_path)
    
    # æµ‹è¯•ç©ºå­—ç¬¦ä¸²
    assert processor.process("") == []
    
    # æµ‹è¯•ç©ºåˆ—è¡¨æ‰¹å¤„ç†
    assert processor.batch_process([]) == []
    
    # æµ‹è¯• None è¾“å…¥
    with pytest.raises(ValueError, match="è¾“å…¥æ–‡æœ¬ä¸èƒ½ä¸º None"):
        processor.process(None)

def test_token_processor_special_characters(model_path):
    """æµ‹è¯•å¤„ç†ç‰¹æ®Šå­—ç¬¦ã€‚"""
    processor = TokenProcessor(model_path=model_path)
    special_chars = "!@#$%^&*()_+-=[]{}|;:'\",.<>?/\\"
    
    tokens = processor.process(special_chars)
    decoded = processor.decode(tokens)
    assert decoded == special_chars

def test_token_processor_unicode(model_path):
    """æµ‹è¯•å¤„ç† Unicode å­—ç¬¦ã€‚"""
    processor = TokenProcessor(model_path=model_path)
    unicode_text = "ä½ å¥½ä¸–ç•ŒğŸŒ"
    
    tokens = processor.process(unicode_text)
    decoded = processor.decode(tokens)
    assert decoded == unicode_text

def test_token_processor_large_input(model_path):
    """æµ‹è¯•å¤„ç†å¤§è¾“å…¥ã€‚"""
    processor = TokenProcessor(model_path=model_path)
    large_text = "Hello world! " * 1000
    
    tokens = processor.process(large_text)
    assert len(tokens) > 1000
    decoded = processor.decode(tokens)
    assert decoded == large_text

def test_token_processor_batch_process_mixed(model_path, large_text_data):
    """æµ‹è¯•æ‰¹å¤„ç†æ··åˆè¾“å…¥ã€‚"""
    processor = TokenProcessor(model_path=model_path)
    results = processor.batch_process(large_text_data)
    
    assert len(results) == len(large_text_data)
    for text, tokens in zip(large_text_data, results):
        decoded = processor.decode(tokens)
        assert decoded == text

def test_token_processor_max_length(model_path):
    """æµ‹è¯•æœ€å¤§é•¿åº¦é™åˆ¶ã€‚"""
    processor = TokenProcessor(model_path=model_path, max_length=10)
    long_text = "This is a very long text that should be truncated"
    
    tokens = processor.process(long_text)
    assert len(tokens) <= 10

# TokenProcessing çš„æ‰©å±•æµ‹è¯•

def test_token_processing_invalid_format(model_path, mock_dataframe):
    """æµ‹è¯•æ— æ•ˆçš„è¾“å‡ºæ ¼å¼ã€‚"""
    processor = TokenProcessing(model_path)
    with pytest.raises(ValueError, match="ä¸æ”¯æŒçš„æ ¼å¼"):
        processor.get_token_data(mock_dataframe, format='invalid')

def test_token_processing_distribution_empty_data(model_path):
    """æµ‹è¯•ç©ºæ•°æ®çš„åˆ†å¸ƒè®¡ç®—ã€‚"""
    processor = TokenProcessing(model_path)
    empty_df = pd.DataFrame(columns=["input_tokens"])
    
    distribution = processor.compute_distribution(empty_df)
    assert isinstance(distribution, dict)
    assert len(distribution) == 0

def test_token_processing_distribution_single_token(model_path):
    """æµ‹è¯•å•ä¸ªä»¤ç‰Œçš„åˆ†å¸ƒè®¡ç®—ã€‚"""
    processor = TokenProcessing(model_path)
    df = pd.DataFrame({
        "input_tokens": [[1]]
    })
    
    distribution = processor.compute_distribution(df)
    assert isinstance(distribution, dict)
    assert len(distribution) == 1
    assert list(distribution.values())[0] == 1.0

def test_token_processing_save_distribution_invalid_path(model_path, mock_dataframe):
    """æµ‹è¯•ä¿å­˜åˆ†å¸ƒå›¾åˆ°å„ç§æ— æ•ˆè·¯å¾„çš„æƒ…å†µã€‚"""
    processor = TokenProcessing(model_path)
    
    # æµ‹è¯•ä¸åŒç±»å‹çš„æ— æ•ˆè·¯å¾„
    invalid_paths = [
        "",  # ç©ºè·¯å¾„
        "invalid",  # æ— æ‰©å±•å
        "plot",  # æ— æ‰©å±•å
        "plot.",  # æ— æ•ˆæ‰©å±•å
        "plot.invalid",  # æ— æ•ˆæ‰©å±•å
        "plot*.png",  # åŒ…å«é€šé…ç¬¦
        "plot?.png",  # åŒ…å«é€šé…ç¬¦
        "plot<>.png",  # åŒ…å«æ— æ•ˆå­—ç¬¦
        "plot|.png",  # åŒ…å«æ— æ•ˆå­—ç¬¦
        "plot\0.png",  # åŒ…å«ç©ºå­—ç¬¦
        "   .png",    # å…¨ç©ºæ ¼æ–‡ä»¶å
        "plot.png ",  # å°¾éƒ¨ç©ºæ ¼
        " plot.png",  # å¼€å¤´ç©ºæ ¼
        ".",         # å½“å‰ç›®å½•
        "..",        # çˆ¶ç›®å½•
    ]
    
    if os.name == 'nt':
        # Windowsç‰¹å®šçš„æ— æ•ˆè·¯å¾„
        invalid_paths.extend([
            "CON.png",  # Windowsä¿ç•™åç§°
            "PRN.png",
            "AUX.png",
            "NUL.png",
            os.path.join("C:", "Windows", "System32", "plot.png"),  # ç³»ç»Ÿç›®å½•
        ])
    
    for invalid_path in invalid_paths:
        with pytest.raises(ValueError):
            processor.compute_distribution(mock_dataframe, save_path=invalid_path)

def test_token_processing_large_dataset(model_path):
    """æµ‹è¯•å¤„ç†å¤§æ•°æ®é›†ã€‚"""
    processor = TokenProcessing(model_path)
    large_df = pd.DataFrame({
        "text": ["Hello world! " * 100] * 100
    })
    
    result = processor.process_tokens(large_df["text"])
    assert len(result) == len(large_df)
    assert all(len(tokens) > 0 for tokens in result["input_tokens"])

def test_token_processing_mixed_data_types(model_path):
    """æµ‹è¯•å¤„ç†æ··åˆæ•°æ®ç±»å‹ã€‚"""
    processor = TokenProcessing(model_path)
    mixed_data = pd.DataFrame({
        "text": [
            "Normal text",
            123,  # æ•°å­—
            True,  # å¸ƒå°”å€¼
            None,  # ç©ºå€¼
            ["list", "of", "items"]  # åˆ—è¡¨
        ]
    })
    
    # åº”è¯¥èƒ½å¤„ç†æ‰€æœ‰ç±»å‹ï¼Œå°†å®ƒä»¬è½¬æ¢ä¸ºå­—ç¬¦ä¸²
    result = processor.process_tokens(mixed_data["text"])
    assert len(result) == len(mixed_data)

def test_token_processing_concurrent_processing(model_path):
    """æµ‹è¯•å¹¶å‘å¤„ç†ã€‚"""
    processor = TokenProcessing(model_path)
    large_df = pd.DataFrame({
        "text": ["Hello world! " * 100] * 100
    })
    
    # ä½¿ç”¨å¤šä¸ªçº¿ç¨‹å¤„ç†
    result1 = processor.process_tokens(large_df["text"])
    result2 = processor.process_tokens(large_df["text"])
    
    # ç»“æœåº”è¯¥ç›¸åŒ
    pd.testing.assert_frame_equal(result1, result2)

def test_token_processing_visualization(model_path, tmp_path):
    """æµ‹è¯•tokenåˆ†å¸ƒå¯è§†åŒ–åŠŸèƒ½"""
    processor = TokenProcessing(model_path)
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    df = pd.DataFrame({
        'input_tokens': [[1, 2, 3], [2, 3, 4], [3, 4, 5]]
    })
    
    # æµ‹è¯•ä¿å­˜åˆ°ä¸´æ—¶ç›®å½•
    save_path = os.path.join(tmp_path, "distribution.png")
    distribution = processor.compute_distribution(df, save_path)
    
    # éªŒè¯ç»“æœ
    assert os.path.exists(save_path)
    assert isinstance(distribution, dict)
    assert len(distribution) > 0
    
def test_token_processing_empty_visualization(model_path, tmp_path):
    """æµ‹è¯•ç©ºæ•°æ®çš„å¯è§†åŒ–å¤„ç†"""
    processor = TokenProcessing(model_path)
    
    # å‡†å¤‡ç©ºæ•°æ®
    df = pd.DataFrame({
        'input_tokens': []
    })
    
    # æµ‹è¯•ä¿å­˜åˆ°ä¸´æ—¶ç›®å½•
    save_path = os.path.join(tmp_path, "empty_distribution.png")
    distribution = processor.compute_distribution(df, save_path)
    
    # éªŒè¯ç»“æœ
    assert isinstance(distribution, dict)
    assert len(distribution) == 0
    
def test_token_processing_invalid_tokens(model_path):
    """æµ‹è¯•æ— æ•ˆtokenæ•°æ®çš„å¤„ç†"""
    processor = TokenProcessing(model_path)
    
    # å‡†å¤‡åŒ…å«Noneå’Œéåˆ—è¡¨æ•°æ®çš„DataFrame
    df = pd.DataFrame({
        'input_tokens': [None, 123, [1, 2, 3], "invalid"]
    })
    
    # æµ‹è¯•åˆ†å¸ƒè®¡ç®—
    distribution = processor.compute_distribution(df)
    
    # éªŒè¯ç»“æœ
    assert isinstance(distribution, dict)
    assert len(distribution) > 0  # åº”è¯¥åªåŒ…å«æœ‰æ•ˆçš„token
    
def test_token_processing_write_permission(model_path, tmp_path):
    """æµ‹è¯•å†™å…¥æƒé™æ£€æŸ¥"""
    processor = TokenProcessing(model_path)
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    df = pd.DataFrame({
        'input_tokens': [[1, 2, 3]]
    })
    
    # åˆ›å»ºä¸€ä¸ªåªè¯»ç›®å½•
    readonly_dir = os.path.join(tmp_path, "readonly")
    os.makedirs(readonly_dir)
    save_path = os.path.join(readonly_dir, "distribution.png")
    
    # åˆ›å»ºä¸€ä¸ªç©ºæ–‡ä»¶å¹¶è®¾ç½®ä¸ºåªè¯»
    with open(save_path, 'w') as f:
        f.write('')
    
    # åœ¨Windowsä¸Šè®¾ç½®æ–‡ä»¶å’Œç›®å½•ä¸ºåªè¯»
    if os.name == 'nt':
        os.system(f'attrib +r "{save_path}"')
        os.system(f'attrib +r "{readonly_dir}"')
    else:
        os.chmod(readonly_dir, 0o444)
        os.chmod(save_path, 0o444)
    
    # æµ‹è¯•å†™å…¥æƒé™æ£€æŸ¥
    with pytest.raises(ValueError, match="æ²¡æœ‰å†™å…¥æƒé™"):
        processor.compute_distribution(df, save_path)
        
def test_token_processing_mixed_columns(model_path):
    """æµ‹è¯•åŒæ—¶åŒ…å«input_tokenså’Œtokenåˆ—çš„æƒ…å†µ"""
    processor = TokenProcessing(model_path)
    
    # å‡†å¤‡åŒ…å«ä¸¤ç§åˆ—çš„æ•°æ®
    df = pd.DataFrame({
        'input_tokens': [[1, 2, 3], [4, 5, 6]],
        'token': ['a', 'b']
    })
    
    # æµ‹è¯•åˆ†å¸ƒè®¡ç®—ï¼ˆåº”è¯¥ä¼˜å…ˆä½¿ç”¨input_tokensï¼‰
    distribution = processor.compute_distribution(df)
    
    # éªŒè¯ç»“æœ
    assert isinstance(distribution, dict)
    assert len(distribution) > 0
    assert 1 in [float(k) for k in distribution.keys()]  # ç¡®è®¤ä½¿ç”¨äº†input_tokensåˆ—
    
def test_token_processing_visualization_error(model_path, tmp_path):
    """æµ‹è¯•å¯è§†åŒ–è¿‡ç¨‹ä¸­çš„é”™è¯¯å¤„ç†"""
    processor = TokenProcessing(model_path)
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    df = pd.DataFrame({
        'input_tokens': [[1, 2, 3]]
    })
    
    # ä½¿ç”¨æ— æ•ˆçš„æ–‡ä»¶åï¼ˆåŒ…å«Windowsä¸Šçš„éæ³•å­—ç¬¦ï¼‰
    save_path = os.path.join(tmp_path, "test<>:.png")
    
    # æµ‹è¯•é”™è¯¯å¤„ç†
    with pytest.raises(ValueError, match="åŒ…å«æ— æ•ˆå­—ç¬¦"):
        processor.compute_distribution(df, save_path)

def test_token_processing_file_permissions(model_path, mock_dataframe, tmp_path):
    """æµ‹è¯•æ–‡ä»¶æƒé™ç›¸å…³çš„åœºæ™¯ã€‚"""
    import os
    import stat
    processor = TokenProcessing(model_path)

    # åˆ›å»ºä¸€ä¸ªåªè¯»ç›®å½•
    readonly_dir = tmp_path / "readonly"
    readonly_dir.mkdir()
    
    # åˆ›å»ºä¸€ä¸ªåªè¯»æ–‡ä»¶
    test_file = readonly_dir / "plot.png"
    test_file.touch()
    
    if os.name == 'nt':
        # Windowsç³»ç»Ÿä¸‹è®¾ç½®ç›®å½•å’Œæ–‡ä»¶ä¸ºåªè¯»
        import subprocess
        subprocess.run(['attrib', '+r', str(readonly_dir)], check=True)
        subprocess.run(['attrib', '+r', str(test_file)], check=True)
    else:
        # Unixç³»ç»Ÿä¸‹è®¾ç½®æƒé™
        readonly_dir.chmod(stat.S_IREAD | stat.S_IXUSR)
        test_file.chmod(stat.S_IREAD)

    try:
        # æµ‹è¯•ä¿å­˜åˆ°åªè¯»ç›®å½•
        save_path = str(test_file)
        with pytest.raises(ValueError, match="æ²¡æœ‰å†™å…¥æƒé™"):
            processor.compute_distribution(mock_dataframe, save_path=save_path)

    finally:
        # æ¢å¤æƒé™ä»¥ä¾¿æ¸…ç†
        if os.name == 'nt':
            subprocess.run(['attrib', '-r', str(readonly_dir)], check=True)
            subprocess.run(['attrib', '-r', str(test_file)], check=True)
        else:
            readonly_dir.chmod(stat.S_IREAD | stat.S_IWRITE | stat.S_IXUSR)
            if test_file.exists():
                test_file.chmod(stat.S_IREAD | stat.S_IWRITE)

def test_token_processing_directory_permissions(model_path, tmp_path):
    """æµ‹è¯•ç›®å½•æƒé™çš„è¯¦ç»†åœºæ™¯ã€‚"""
    import os
    import stat
    processor = TokenProcessing(model_path)
    df = pd.DataFrame({"input_tokens": [[1, 2, 3]]})

    # åˆ›å»ºåµŒå¥—çš„ç›®å½•ç»“æ„
    nested_dir = tmp_path / "parent" / "child"
    nested_dir.mkdir(parents=True)
    
    if os.name == 'nt':
        # Windowsç³»ç»Ÿä¸‹è®¾ç½®ç›®å½•ä¸ºåªè¯»
        import subprocess
        subprocess.run(['attrib', '+r', str(nested_dir.parent)], check=True)
    else:
        # Unixç³»ç»Ÿä¸‹è®¾ç½®æƒé™
        nested_dir.parent.chmod(stat.S_IREAD | stat.S_IXUSR)

    try:
        save_path = str(nested_dir / "plot.png")
        with pytest.raises(ValueError, match="æ²¡æœ‰å†™å…¥æƒé™"):
            processor.compute_distribution(df, save_path=save_path)
    finally:
        # æ¢å¤æƒé™ä»¥ä¾¿æ¸…ç†
        if os.name == 'nt':
            subprocess.run(['attrib', '-r', str(nested_dir.parent)], check=True)
        else:
            nested_dir.parent.chmod(stat.S_IREAD | stat.S_IWRITE | stat.S_IXUSR)

def test_token_processing_path_validation(model_path, tmp_path):
    """æµ‹è¯•è·¯å¾„éªŒè¯é€»è¾‘çš„å„ç§åœºæ™¯ã€‚"""
    processor = TokenProcessing(model_path)
    df = pd.DataFrame({"input_tokens": [[1, 2, 3]]})
    
    # æµ‹è¯•ç›¸å¯¹è·¯å¾„
    relative_path = "plot.png"
    with pytest.raises(ValueError, match="å¿…é¡»ä½¿ç”¨ç»å¯¹è·¯å¾„"):
        processor.compute_distribution(df, save_path=relative_path)
    
    # æµ‹è¯•ä¸å­˜åœ¨çš„ç›®å½•
    nonexistent_dir = os.path.join(tmp_path, "nonexistent", "plot.png")
    with pytest.raises(ValueError, match="ç›®å½•ä¸å­˜åœ¨"):
        processor.compute_distribution(df, save_path=nonexistent_dir)
    
    # æµ‹è¯•ç‰¹æ®Šå­—ç¬¦è·¯å¾„ï¼ˆWindowsç‰¹å®šï¼‰
    if os.name == 'nt':
        invalid_chars = ['<', '>', ':', '"', '|', '?', '*']
        for char in invalid_chars:
            invalid_path = os.path.join(tmp_path, f"test{char}plot.png")
            with pytest.raises(ValueError, match="åŒ…å«æ— æ•ˆå­—ç¬¦"):
                processor.compute_distribution(df, save_path=invalid_path)
    
    # æµ‹è¯•æœ‰æ•ˆçš„ç»å¯¹è·¯å¾„
    valid_path = os.path.join(tmp_path, "plot.png")
    distribution = processor.compute_distribution(df, save_path=valid_path)
    assert os.path.exists(valid_path)
    assert isinstance(distribution, dict)

def test_token_processing_long_path(model_path, tmp_path):
    """æµ‹è¯•é•¿è·¯å¾„åœºæ™¯ã€‚"""
    processor = TokenProcessing(model_path)
    df = pd.DataFrame({"input_tokens": [[1, 2, 3]]})
    
    # åˆ›å»ºä¸€ä¸ªéå¸¸é•¿çš„è·¯å¾„
    long_path_components = ["subfolder"] * 50
    long_path = tmp_path
    for component in long_path_components:
        long_path = long_path / component
    
    # åœ¨Windowsç³»ç»Ÿä¸‹ä½¿ç”¨é•¿è·¯å¾„å‰ç¼€
    if os.name == 'nt':
        long_path_str = str(long_path)
        if len(long_path_str) > 260:
            long_path_str = '\\\\?\\' + long_path_str
        long_path = Path(long_path_str)
    
    long_path.mkdir(parents=True, exist_ok=True)
    
    save_path = str(long_path / "plot.png")
    
    if os.name == 'nt' and len(save_path) > 260:  # Windows MAX_PATHé™åˆ¶
        with pytest.raises(ValueError, match="è·¯å¾„é•¿åº¦è¶…è¿‡ç³»ç»Ÿé™åˆ¶"):
            processor.compute_distribution(df, save_path=save_path)
    else:
        distribution = processor.compute_distribution(df, save_path=save_path)
        assert os.path.exists(save_path)
        assert isinstance(distribution, dict)

def test_token_processing_unc_path(model_path):
    """æµ‹è¯•UNCè·¯å¾„åœºæ™¯ã€‚"""
    processor = TokenProcessing(model_path)
    df = pd.DataFrame({"input_tokens": [[1, 2, 3]]})
    
    # ä½¿ç”¨ä¸€ä¸ªæ— æ•ˆçš„UNCè·¯å¾„
    if os.name == 'nt':
        unc_path = r"\\nonexistent\share\plot.png"
        with pytest.raises(ValueError, match="æ— æ³•è®¿é—®ç½‘ç»œè·¯å¾„"):
            processor.compute_distribution(df, save_path=unc_path)

def test_token_processing_unicode_path(model_path, tmp_path):
    """æµ‹è¯•Unicodeè·¯å¾„åœºæ™¯ã€‚"""
    processor = TokenProcessing(model_path)
    df = pd.DataFrame({"input_tokens": [[1, 2, 3]]})
    
    # æµ‹è¯•ä¸åŒè¯­è¨€çš„æ–‡ä»¶å
    test_paths = {
        "ä¸­æ–‡è·¯å¾„": tmp_path / "æµ‹è¯•" / "åˆ†å¸ƒå›¾.png",
        "æ—¥æ–‡è·¯å¾„": tmp_path / "ãƒ†ã‚¹ãƒˆ" / "å›³.png",
        "éŸ©æ–‡è·¯å¾„": tmp_path / "í…ŒìŠ¤íŠ¸" / "ê·¸ë˜í”„.png",
        "ä¿„æ–‡è·¯å¾„": tmp_path / "Ñ‚ĞµÑÑ‚" / "Ğ³Ñ€Ğ°Ñ„Ğ¸Ğº.png"
    }
    
    for name, path in test_paths.items():
        path.parent.mkdir(parents=True, exist_ok=True)
        distribution = processor.compute_distribution(df, save_path=str(path))
        assert os.path.exists(path), f"{name}åˆ›å»ºå¤±è´¥"
        assert isinstance(distribution, dict)

def test_token_processing_reserved_names(mock_dataframe, tmp_path):
    """æµ‹è¯•Windowsä¿ç•™æ–‡ä»¶åçš„å¤„ç†ã€‚
    
    éªŒè¯ä½¿ç”¨Windowsä¿ç•™åç§°ï¼ˆå¦‚CONã€PRNç­‰ï¼‰ä½œä¸ºæ–‡ä»¶åæ—¶æ˜¯å¦æ­£ç¡®æŠ›å‡ºå¼‚å¸¸ã€‚
    """
    processor = TokenProcessing("mock_model_path")
    distribution = processor.compute_distribution(mock_dataframe)
    
    reserved_names = ['CON.png', 'PRN.png', 'AUX.png', 'NUL.png', 'COM1.png']
    for name in reserved_names:
        save_path = os.path.join(tmp_path, name)
        with pytest.raises(ValueError, match="æ–‡ä»¶åä¸èƒ½ä½¿ç”¨Windowsä¿ç•™åç§°"):
            processor._save_distribution_plot(distribution, save_path)

def test_token_processing_empty_path(mock_dataframe):
    """æµ‹è¯•ç©ºè·¯å¾„å’Œç©ºç™½å­—ç¬¦è·¯å¾„çš„å¤„ç†ã€‚
    
    éªŒè¯ç©ºè·¯å¾„ã€ç©ºç™½å­—ç¬¦è·¯å¾„å’ŒNoneå€¼æ˜¯å¦æ­£ç¡®æŠ›å‡ºå¼‚å¸¸ã€‚
    """
    processor = TokenProcessing("mock_model_path")
    distribution = processor.compute_distribution(mock_dataframe)
    
    invalid_paths = ['', ' ', '  ', None]
    for path in invalid_paths:
        with pytest.raises(ValueError, match="ä¿å­˜è·¯å¾„ä¸èƒ½ä¸ºç©º"):
            processor._save_distribution_plot(distribution, path)

def test_token_processing_extension_validation(mock_dataframe, tmp_path):
    """æµ‹è¯•æ–‡ä»¶æ‰©å±•åéªŒè¯ã€‚
    
    éªŒè¯ï¼š
    1. æ— æ‰©å±•åæ–‡ä»¶
    2. é”™è¯¯æ‰©å±•åï¼ˆé.pngï¼‰
    3. å¤§å†™PNGæ‰©å±•å
    """
    processor = TokenProcessing("mock_model_path")
    distribution = processor.compute_distribution(mock_dataframe)
    
    # æµ‹è¯•æ— æ‰©å±•å
    no_ext_path = os.path.join(tmp_path, "no_extension")
    with pytest.raises(ValueError, match="å¿…é¡»æ˜¯.pngæ ¼å¼"):
        processor._save_distribution_plot(distribution, no_ext_path)
    
    # æµ‹è¯•é”™è¯¯æ‰©å±•å
    invalid_exts = ['.jpg', '.pdf', '.txt']
    for ext in invalid_exts:
        invalid_path = os.path.join(tmp_path, f"test{ext}")
        with pytest.raises(ValueError, match="å¿…é¡»æ˜¯.pngæ ¼å¼"):
            processor._save_distribution_plot(distribution, invalid_path)
    
    # æµ‹è¯•å¤§å†™PNGæ‰©å±•åï¼ˆåº”è¯¥æˆåŠŸï¼‰
    valid_path = os.path.join(tmp_path, "test.PNG")
    processor._save_distribution_plot(distribution, valid_path)
    assert os.path.exists(valid_path)
    assert isinstance(distribution, dict)

def test_token_processing_process_tokens(model_path):
    """æµ‹è¯•process_tokensæ–¹æ³•çš„å„ç§è¾“å…¥æƒ…å†µã€‚"""
    processor = TokenProcessing(model_path)
    
    # æµ‹è¯•ç©ºè¾“å…¥
    empty_result = processor.process_tokens([])
    assert isinstance(empty_result, pd.DataFrame)
    assert empty_result.empty
    assert list(empty_result.columns) == ["input_tokens", "decoded_text"]
    
    # æµ‹è¯•Seriesè¾“å…¥
    series_input = pd.Series(["Hello", "World"])
    series_result = processor.process_tokens(series_input)
    assert isinstance(series_result, pd.DataFrame)
    assert len(series_result) == 2
    
    # æµ‹è¯•Noneå€¼å¤„ç†
    none_result = processor.process_tokens([None, "Valid"])
    assert isinstance(none_result, pd.DataFrame)
    assert len(none_result) == 2
    assert none_result["input_tokens"].iloc[0] == []
    assert none_result["decoded_text"].iloc[0] == ""
    
    # æµ‹è¯•éå­—ç¬¦ä¸²ç±»å‹
    mixed_result = processor.process_tokens([123, True, "Text"])
    assert isinstance(mixed_result, pd.DataFrame)
    assert len(mixed_result) == 3
    assert all(isinstance(text, str) for text in mixed_result["decoded_text"])

def test_token_processing_get_token_data(model_path, mock_dataframe):
    """æµ‹è¯•get_token_dataæ–¹æ³•çš„å„ç§æƒ…å†µã€‚"""
    processor = TokenProcessing(model_path)
    
    # æµ‹è¯•æ— æ•ˆæ ¼å¼
    with pytest.raises(ValueError, match="ä¸æ”¯æŒçš„æ ¼å¼"):
        processor.get_token_data(mock_dataframe, format='invalid')
    
    # æµ‹è¯•ç©ºDataFrame
    empty_df = pd.DataFrame()
    empty_result = processor.get_token_data(empty_df)
    assert isinstance(empty_result, pd.DataFrame)
    assert empty_result.empty
    
    # æµ‹è¯•dictæ ¼å¼è¾“å‡º
    dict_result = processor.get_token_data(mock_dataframe, format='dict')
    assert isinstance(dict_result, dict)
    assert "input_tokens" in dict_result
    assert "decoded_text" in dict_result
    
    # æµ‹è¯•æ··åˆåˆ—å
    mixed_df = pd.DataFrame({
        "token": [[1, 2, 3]],
        "decoded_text": ["Test"]
    })
    mixed_result = processor.get_token_data(mixed_df)
    assert isinstance(mixed_result, pd.DataFrame)
    assert "input_tokens" in mixed_result.columns
    assert "decoded_text" in mixed_result.columns

def test_data_processor_validate_data():
    """æµ‹è¯•DataProcessorçš„validate_dataæ–¹æ³•ã€‚"""
    processor = DataProcessor()
    
    # æµ‹è¯•Noneè¾“å…¥
    with pytest.raises(ValueError, match="è¾“å…¥æ•°æ®ä¸èƒ½ä¸ºNone"):
        processor.validate_data(None)
    
    # æµ‹è¯•ç©ºåˆ—è¡¨
    with pytest.raises(ValueError, match="è¾“å…¥æ•°æ®åˆ—è¡¨ä¸èƒ½ä¸ºç©º"):
        processor.validate_data([])
    
    # æµ‹è¯•æ— æ•ˆç±»å‹
    with pytest.raises(TypeError, match="è¾“å…¥æ•°æ®å¿…é¡»æ˜¯DataFrameã€å­—å…¸æˆ–å­—å…¸åˆ—è¡¨"):
        processor.validate_data("invalid")
    
    # æµ‹è¯•å•ä¸ªå­—å…¸
    single_dict = {"text": "test", "tokens": [1, 2, 3], "length": 3}
    result = processor.validate_data(single_dict)
    assert isinstance(result, pd.DataFrame)
    assert len(result) == 1
    
    # æµ‹è¯•å­—å…¸åˆ—è¡¨
    dict_list = [
        {"text": "test1", "tokens": [1, 2], "length": 2},
        {"text": "test2", "tokens": [3, 4], "length": 2}
    ]
    result = processor.validate_data(dict_list)
    assert isinstance(result, pd.DataFrame)
    assert len(result) == 2
    
    # æµ‹è¯•ç¼ºå°‘å¿…éœ€åˆ—
    invalid_df = pd.DataFrame({"text": ["test"]})
    with pytest.raises(ValueError, match="ç¼ºå°‘å¿…éœ€çš„åˆ—"):
        processor.validate_data(invalid_df)

def test_data_processor_process_batch():
    """æµ‹è¯•DataProcessorçš„process_batchæ–¹æ³•ã€‚"""
    processor = DataProcessor()
    
    # æµ‹è¯•æ— æ•ˆbatch_size
    with pytest.raises(ValueError, match="batch_sizeå¿…é¡»æ˜¯æ­£æ•´æ•°"):
        processor.process_batch(pd.DataFrame(), batch_size=0)
    
    # æµ‹è¯•æ­£å¸¸æ‰¹å¤„ç†
    data = pd.DataFrame({
        "text": ["test1", "test2", "test3"],
        "tokens": [[1, 2], [3, 4], [5, 6]],
        "length": [2, 2, 2]
    })
    
    batches = list(processor.process_batch(data, batch_size=2))
    assert len(batches) == 2
    assert len(batches[0]) == 2
    assert len(batches[1]) == 1

def test_alpaca_loader_validate_entry():
    """æµ‹è¯•AlpacaLoaderçš„validate_entryæ–¹æ³•ã€‚"""
    loader = AlpacaLoader("dummy_path")
    
    # æµ‹è¯•æœ‰æ•ˆæ¡ç›®
    valid_entry = {
        "instruction": "Test instruction",
        "input": "Test input",
        "output": "Test output"
    }
    assert loader.validate_entry(valid_entry) is True
    
    # æµ‹è¯•ç¼ºå°‘å­—æ®µ
    missing_field = {"instruction": "Test"}
    assert loader.validate_entry(missing_field) is False
    
    # æµ‹è¯•éå­—ç¬¦ä¸²å­—æ®µ
    invalid_type = {
        "instruction": 123,
        "input": "Test",
        "output": "Test"
    }
    assert loader.validate_entry(invalid_type) is False
    
    # æµ‹è¯•ç©ºinstruction
    empty_instruction = {
        "instruction": "",
        "input": "Test",
        "output": "Test"
    }
    assert loader.validate_entry(empty_instruction) is False

def test_alpaca_loader_get_statistics():
    """æµ‹è¯•AlpacaLoaderçš„get_statisticsæ–¹æ³•ã€‚"""
    # åˆ›å»ºä¸´æ—¶æµ‹è¯•æ–‡ä»¶
    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
        test_data = [
            {
                "instruction": "Test instruction 1",
                "input": "Test input 1",
                "output": "Test output 1"
            },
            {
                "instruction": "Test instruction 2",
                "input": "",
                "output": "Test output 2"
            }
        ]
        json.dump(test_data, f)
        test_file = f.name
    
    try:
        loader = AlpacaLoader(test_file)
        stats = loader.get_statistics()
        
        assert isinstance(stats, dict)
        assert "total_samples" in stats
        assert "avg_instruction_length" in stats
        assert "avg_input_length" in stats
        assert "avg_output_length" in stats
        assert "empty_input_ratio" in stats
        assert stats["total_samples"] == 2
        assert stats["empty_input_ratio"] == 0.5
    finally:
        os.unlink(test_file)

@pytest.fixture
def mock_dataframe():
    """åˆ›å»ºæ¨¡æ‹ŸDataFrameã€‚"""
    return pd.DataFrame({
        "text": ["Hello", "Hi", "Hey"],
        "input_tokens": [[1, 2, 3], [4, 5], [6, 7, 8]],
        "decoded_text": ["Hello", "Hi", "Hey"]
    })

def test_token_processing_large_distribution(model_path, tmp_path):
    """æµ‹è¯•å¤„ç†å¤§é‡tokençš„åˆ†å¸ƒè®¡ç®—ã€‚"""
    processor = TokenProcessing(model_path)
    
    # åˆ›å»ºåŒ…å«å¤§é‡tokençš„DataFrame
    large_df = pd.DataFrame({
        "input_tokens": [[i] * 1000 for i in range(100)]  # 100,000ä¸ªtoken
    })
    
    # æµ‹è¯•è®¡ç®—åˆ†å¸ƒ
    distribution = processor.compute_distribution(large_df)
    assert isinstance(distribution, dict)
    assert len(distribution) == 100
    assert all(isinstance(k, str) for k in distribution.keys())
    assert all(isinstance(v, float) for v in distribution.values())
    
    # æµ‹è¯•ä¿å­˜å¤§åˆ†å¸ƒå›¾
    save_path = os.path.join(tmp_path, "large_distribution.png")
    processor.compute_distribution(large_df, save_path=save_path)
    assert os.path.exists(save_path)

def test_token_processing_concurrent_operations(model_path, tmp_path):
    """æµ‹è¯•å¹¶å‘æ“ä½œåœºæ™¯ã€‚"""
    processor = TokenProcessing(model_path)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    df = pd.DataFrame({
        "input_tokens": [[1, 2, 3], [2, 3, 4], [3, 4, 5]]
    })
    
    # åŒæ—¶è¿›è¡Œå¤šä¸ªæ“ä½œ
    save_path1 = os.path.join(tmp_path, "dist1.png")
    save_path2 = os.path.join(tmp_path, "dist2.png")
    
    # è®¡ç®—åˆ†å¸ƒå¹¶ä¿å­˜
    dist1 = processor.compute_distribution(df, save_path=save_path1)
    dist2 = processor.compute_distribution(df, save_path=save_path2)
    
    # éªŒè¯ç»“æœ
    assert os.path.exists(save_path1)
    assert os.path.exists(save_path2)
    assert dist1 == dist2

def test_token_processing_error_handling(model_path):
    """æµ‹è¯•é”™è¯¯å¤„ç†åœºæ™¯ã€‚"""
    processor = TokenProcessing(model_path)
    
    # æµ‹è¯•æ— æ•ˆçš„tokenæ•°æ®
    invalid_df = pd.DataFrame({
        "input_tokens": [None, "invalid", [1, 2, 3]]
    })
    
    # åº”è¯¥èƒ½å¤Ÿå¤„ç†æ— æ•ˆæ•°æ®å¹¶è¿”å›ç©ºåˆ†å¸ƒ
    distribution = processor.compute_distribution(invalid_df)
    assert isinstance(distribution, dict)
    assert len(distribution) > 0  # åº”è¯¥åªåŒ…å«æœ‰æ•ˆtoken
    
    # æµ‹è¯•æ–‡ä»¶ç³»ç»Ÿé”™è¯¯
    with pytest.raises(ValueError):
        processor.compute_distribution(
            invalid_df,
            save_path="/nonexistent/directory/plot.png"
        )

def test_token_processing_memory_usage(model_path):
    """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µã€‚"""
    processor = TokenProcessing(model_path)
    
    # åˆ›å»ºå¤§é‡æ•°æ®
    large_data = ["test" * 1000] * 1000  # 1MBæ–‡æœ¬æ•°æ®
    
    # å¤„ç†å¤§é‡æ•°æ®
    result = processor.process_tokens(large_data)
    
    # éªŒè¯ç»“æœ
    assert isinstance(result, pd.DataFrame)
    assert len(result) == len(large_data)
    assert "input_tokens" in result.columns
    assert "decoded_text" in result.columns

def test_token_processing_performance(model_path):
    """æµ‹è¯•æ€§èƒ½ç›¸å…³åœºæ™¯ã€‚"""
    processor = TokenProcessing(model_path)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_data = ["test"] * 1000
    
    # æµ‹é‡å¤„ç†æ—¶é—´
    start_time = time.time()
    result = processor.process_tokens(test_data)
    end_time = time.time()
    
    # éªŒè¯ç»“æœ
    assert isinstance(result, pd.DataFrame)
    assert len(result) == len(test_data)
    
    # è®°å½•å¤„ç†æ—¶é—´
    processing_time = end_time - start_time
    assert processing_time < 5.0  # å‡è®¾å¤„ç†1000ä¸ªæ ·æœ¬åº”è¯¥åœ¨5ç§’å†…å®Œæˆ

def test_token_processing_resource_cleanup(model_path, tmp_path):
    """æµ‹è¯•èµ„æºæ¸…ç†ã€‚"""
    processor = TokenProcessing(model_path)
    
    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
    temp_file = os.path.join(tmp_path, "temp.png")
    
    # æ‰§è¡Œæ“ä½œ
    df = pd.DataFrame({"input_tokens": [[1, 2, 3]]})
    processor.compute_distribution(df, save_path=temp_file)
    
    # éªŒè¯æ–‡ä»¶åˆ›å»º
    assert os.path.exists(temp_file)
    
    # æ¸…ç†èµ„æº
    processor.cleanup()
    
    # éªŒè¯èµ„æºå·²æ¸…ç†
    assert not hasattr(processor, 'tokenizer')

def test_token_processing_special_characters(model_path):
    """æµ‹è¯•ç‰¹æ®Šå­—ç¬¦å¤„ç†ã€‚"""
    processor = TokenProcessing(model_path)
    
    # æµ‹è¯•å„ç§ç‰¹æ®Šå­—ç¬¦
    special_chars = [
        "!@#$%^&*()_+-=[]{}|;:'\",.<>?/\\",  # æ ‡ç‚¹ç¬¦å·
        "ä½ å¥½ä¸–ç•ŒğŸŒ",  # Unicodeå­—ç¬¦
        "æµ‹è¯•\tæ¢è¡Œ\næµ‹è¯•",  # ç©ºç™½å­—ç¬¦
        "æµ‹è¯•\r\nWindowsæ¢è¡Œ",  # ä¸åŒç³»ç»Ÿçš„æ¢è¡Œç¬¦
        "æµ‹è¯•\x00ç©ºå­—ç¬¦",  # æ§åˆ¶å­—ç¬¦
    ]
    
    # å¤„ç†ç‰¹æ®Šå­—ç¬¦
    result = processor.process_tokens(special_chars)
    
    # éªŒè¯ç»“æœ
    assert isinstance(result, pd.DataFrame)
    assert len(result) == len(special_chars)
    assert all(isinstance(tokens, list) for tokens in result["input_tokens"])
    assert all(isinstance(text, str) for text in result["decoded_text"])

def test_token_processing_empty_tokens(model_path):
    """æµ‹è¯•ç©ºtokenå¤„ç†ã€‚"""
    processor = TokenProcessing(model_path)
    
    # æµ‹è¯•å„ç§ç©ºtokenæƒ…å†µ
    empty_cases = [
        [],  # ç©ºåˆ—è¡¨
        [""],  # ç©ºå­—ç¬¦ä¸²
        [None],  # Noneå€¼
        [[]],  # ç©ºå­åˆ—è¡¨
        [[], []],  # å¤šä¸ªç©ºåˆ—è¡¨
    ]
    
    # å¤„ç†ç©ºtoken
    for case in empty_cases:
        df = pd.DataFrame({"input_tokens": case})
        distribution = processor.compute_distribution(df)
        assert isinstance(distribution, dict)
        assert len(distribution) == 0

def test_token_processing_mixed_data_types(model_path):
    """æµ‹è¯•æ··åˆæ•°æ®ç±»å‹å¤„ç†ã€‚"""
    processor = TokenProcessing(model_path)
    
    # åˆ›å»ºæ··åˆæ•°æ®ç±»å‹çš„DataFrame
    mixed_df = pd.DataFrame({
        "input_tokens": [
            [1, 2, 3],  # æ•´æ•°åˆ—è¡¨
            ["1", "2", "3"],  # å­—ç¬¦ä¸²åˆ—è¡¨
            [1.0, 2.0, 3.0],  # æµ®ç‚¹æ•°åˆ—è¡¨
            [True, False],  # å¸ƒå°”å€¼åˆ—è¡¨
            [None, None],  # Noneå€¼åˆ—è¡¨
        ]
    })
    
    # å¤„ç†æ··åˆæ•°æ®
    distribution = processor.compute_distribution(mixed_df)
    
    # éªŒè¯ç»“æœ
    assert isinstance(distribution, dict)
    assert all(isinstance(k, str) for k in distribution.keys())
    assert all(isinstance(v, float) for v in distribution.values())

def test_token_processing_duplicate_tokens(model_path):
    """æµ‹è¯•é‡å¤tokenå¤„ç†ã€‚"""
    processor = TokenProcessing(model_path)
    
    # åˆ›å»ºåŒ…å«é‡å¤tokençš„æ•°æ®
    duplicate_df = pd.DataFrame({
        "input_tokens": [
            [1, 1, 1],  # è¿ç»­é‡å¤
            [1, 2, 1],  # é—´éš”é‡å¤
            [1, 1, 2, 2],  # å¤šç»„é‡å¤
        ]
    })
    
    # è®¡ç®—åˆ†å¸ƒ
    distribution = processor.compute_distribution(duplicate_df)
    
    # éªŒè¯ç»“æœ
    assert isinstance(distribution, dict)
    assert len(distribution) == 2  # åº”è¯¥åªæœ‰ä¸¤ä¸ªä¸åŒçš„token
    assert sum(distribution.values()) == 1.0  # æ¦‚ç‡å’Œåº”è¯¥ä¸º1

def test_token_processing_nested_tokens(model_path):
    """æµ‹è¯•åµŒå¥—tokenå¤„ç†ã€‚"""
    processor = TokenProcessing(model_path)
    
    # åˆ›å»ºåµŒå¥—tokenæ•°æ®
    nested_df = pd.DataFrame({
        "input_tokens": [
            [[1, 2], [3, 4]],  # ä¸¤å±‚åµŒå¥—
            [[[1]], [[2]]],  # ä¸‰å±‚åµŒå¥—
            [1, [2, [3]]],  # æ··åˆåµŒå¥—
        ]
    })
    
    # å¤„ç†åµŒå¥—æ•°æ®
    distribution = processor.compute_distribution(nested_df)
    
    # éªŒè¯ç»“æœ
    assert isinstance(distribution, dict)
    assert all(isinstance(k, str) for k in distribution.keys())
    assert all(isinstance(v, float) for v in distribution.values())

def test_token_processing_large_numbers(model_path):
    """æµ‹è¯•å¤§æ•°å­—å¤„ç†ã€‚"""
    processor = TokenProcessing(model_path)
    
    # åˆ›å»ºåŒ…å«å¤§æ•°å­—çš„æ•°æ®
    large_numbers_df = pd.DataFrame({
        "input_tokens": [
            [2**32],  # 32ä½æ•´æ•°
            [2**64],  # 64ä½æ•´æ•°
            [1e100],  # å¤§æµ®ç‚¹æ•°
            [-2**32],  # è´Ÿå¤§æ•´æ•°
        ]
    })
    
    # å¤„ç†å¤§æ•°å­—
    distribution = processor.compute_distribution(large_numbers_df)
    
    # éªŒè¯ç»“æœ
    assert isinstance(distribution, dict)
    assert all(isinstance(k, str) for k in distribution.keys())
    assert all(isinstance(v, float) for v in distribution.values())

def test_token_processing_unicode_normalization(model_path):
    """æµ‹è¯•Unicodeæ ‡å‡†åŒ–å¤„ç†ã€‚"""
    processor = TokenProcessing(model_path)
    
    # æµ‹è¯•Unicodeç»„åˆå­—ç¬¦
    unicode_cases = [
        "cafÃ©",  # å¸¦é‡éŸ³
        "cafe\u0301",  # ç»„åˆé‡éŸ³
        "ä½ å¥½",  # ä¸­æ–‡å­—ç¬¦
        "ã“ã‚“ã«ã¡ã¯",  # æ—¥æ–‡å­—ç¬¦
        "ì•ˆë…•í•˜ì„¸ìš”",  # éŸ©æ–‡å­—ç¬¦
    ]
    
    # å¤„ç†Unicodeå­—ç¬¦
    result = processor.process_tokens(unicode_cases)
    
    # éªŒè¯ç»“æœ
    assert isinstance(result, pd.DataFrame)
    assert len(result) == len(unicode_cases)
    assert all(isinstance(tokens, list) for tokens in result["input_tokens"])
    assert all(isinstance(text, str) for text in result["decoded_text"]) 