"""TokenProcessor å’Œ TokenProcessing çš„æ‰©å±•æµ‹è¯•ç”¨ä¾‹ã€‚"""

import os
os.environ['TEST_MODE'] = '1'

import pytest
import pandas as pd
import numpy as np
from pathlib import Path
import json
from src.data_processing.token_processor import TokenProcessor, MockTokenizer
from src.data_processing.token_processing import TokenProcessing
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯

@pytest.fixture
def model_path(tmp_path):
    """åˆ›å»ºæµ‹è¯•ç”¨çš„æ¨¡å‹è·¯å¾„ã€‚"""
    model_dir = tmp_path / "models" / "TinyLlama-1.1B-Chat-v1.0"
    model_dir.mkdir(parents=True)
    return str(model_dir)

@pytest.fixture
def invalid_model_path(tmp_path):
    """åˆ›å»ºæ— æ•ˆçš„æ¨¡å‹è·¯å¾„ã€‚"""
    return str(tmp_path / "nonexistent_model")

@pytest.fixture
def large_text_data():
    """åˆ›å»ºå¤§æ–‡æœ¬æ•°æ®ã€‚"""
    return ["Hello world! " * 1000,  # é•¿æ–‡æœ¬
            "",  # ç©ºæ–‡æœ¬
            "Special chars: !@#$%^&*()",  # ç‰¹æ®Šå­—ç¬¦
            "Numbers: 1234567890",  # æ•°å­—
            "Unicode: ä½ å¥½ä¸–ç•ŒğŸŒ"  # Unicodeå­—ç¬¦
            ]

# TokenProcessor çš„æ‰©å±•æµ‹è¯•

def test_token_processor_invalid_model_path(invalid_model_path):
    """æµ‹è¯•ä½¿ç”¨æ— æ•ˆçš„æ¨¡å‹è·¯å¾„åˆå§‹åŒ– TokenProcessorã€‚"""
    with pytest.raises(ValueError, match="æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨"):
        TokenProcessor(model_path=invalid_model_path, validate_path=True)

def test_token_processor_empty_input(model_path):
    """æµ‹è¯•å¤„ç†ç©ºè¾“å…¥ã€‚"""
    processor = TokenProcessor(model_path=model_path)
    
    # æµ‹è¯•ç©ºå­—ç¬¦ä¸²
    assert processor.process("") == []
    
    # æµ‹è¯•ç©ºåˆ—è¡¨æ‰¹å¤„ç†
    assert processor.batch_process([]) == []
    
    # æµ‹è¯• None è¾“å…¥
    with pytest.raises(ValueError, match="è¾“å…¥æ–‡æœ¬ä¸èƒ½ä¸º None"):
        processor.process(None)

def test_token_processor_special_characters(model_path):
    """æµ‹è¯•å¤„ç†ç‰¹æ®Šå­—ç¬¦ã€‚"""
    processor = TokenProcessor(model_path=model_path)
    special_chars = "!@#$%^&*()_+-=[]{}|;:'\",.<>?/\\"
    
    tokens = processor.process(special_chars)
    decoded = processor.decode(tokens)
    assert decoded == special_chars

def test_token_processor_unicode(model_path):
    """æµ‹è¯•å¤„ç† Unicode å­—ç¬¦ã€‚"""
    processor = TokenProcessor(model_path=model_path)
    unicode_text = "ä½ å¥½ä¸–ç•ŒğŸŒ"
    
    tokens = processor.process(unicode_text)
    decoded = processor.decode(tokens)
    assert decoded == unicode_text

def test_token_processor_large_input(model_path):
    """æµ‹è¯•å¤„ç†å¤§è¾“å…¥ã€‚"""
    processor = TokenProcessor(model_path=model_path)
    large_text = "Hello world! " * 1000
    
    tokens = processor.process(large_text)
    assert len(tokens) > 1000
    decoded = processor.decode(tokens)
    assert decoded == large_text

def test_token_processor_batch_process_mixed(model_path, large_text_data):
    """æµ‹è¯•æ‰¹å¤„ç†æ··åˆè¾“å…¥ã€‚"""
    processor = TokenProcessor(model_path=model_path)
    results = processor.batch_process(large_text_data)
    
    assert len(results) == len(large_text_data)
    for text, tokens in zip(large_text_data, results):
        decoded = processor.decode(tokens)
        assert decoded == text

def test_token_processor_max_length(model_path):
    """æµ‹è¯•æœ€å¤§é•¿åº¦é™åˆ¶ã€‚"""
    processor = TokenProcessor(model_path=model_path, max_length=10)
    long_text = "This is a very long text that should be truncated"
    
    tokens = processor.process(long_text)
    assert len(tokens) <= 10

# TokenProcessing çš„æ‰©å±•æµ‹è¯•

def test_token_processing_invalid_format(model_path, mock_dataframe):
    """æµ‹è¯•æ— æ•ˆçš„è¾“å‡ºæ ¼å¼ã€‚"""
    processor = TokenProcessing(model_path)
    with pytest.raises(ValueError, match="ä¸æ”¯æŒçš„æ ¼å¼"):
        processor.get_token_data(mock_dataframe, format='invalid')

def test_token_processing_distribution_empty_data(model_path):
    """æµ‹è¯•ç©ºæ•°æ®çš„åˆ†å¸ƒè®¡ç®—ã€‚"""
    processor = TokenProcessing(model_path)
    empty_df = pd.DataFrame(columns=["input_tokens"])
    
    distribution = processor.compute_distribution(empty_df)
    assert isinstance(distribution, dict)
    assert len(distribution) == 0

def test_token_processing_distribution_single_token(model_path):
    """æµ‹è¯•å•ä¸ªä»¤ç‰Œçš„åˆ†å¸ƒè®¡ç®—ã€‚"""
    processor = TokenProcessing(model_path)
    df = pd.DataFrame({
        "input_tokens": [[1]]
    })
    
    distribution = processor.compute_distribution(df)
    assert isinstance(distribution, dict)
    assert len(distribution) == 1
    assert list(distribution.values())[0] == 1.0

def test_token_processing_save_distribution_invalid_path(model_path, mock_dataframe):
    """æµ‹è¯•ä¿å­˜åˆ†å¸ƒå›¾åˆ°æ— æ•ˆè·¯å¾„ã€‚"""
    processor = TokenProcessing(model_path)
    invalid_path = "/nonexistent/directory/plot.png"
    
    with pytest.raises(Exception):
        processor.compute_distribution(mock_dataframe, save_path=invalid_path)

def test_token_processing_large_dataset(model_path):
    """æµ‹è¯•å¤„ç†å¤§æ•°æ®é›†ã€‚"""
    processor = TokenProcessing(model_path)
    large_df = pd.DataFrame({
        "text": ["Hello world! " * 100] * 100
    })
    
    result = processor.process_tokens(large_df["text"])
    assert len(result) == len(large_df)
    assert all(len(tokens) > 0 for tokens in result["input_tokens"])

def test_token_processing_mixed_data_types(model_path):
    """æµ‹è¯•å¤„ç†æ··åˆæ•°æ®ç±»å‹ã€‚"""
    processor = TokenProcessing(model_path)
    mixed_data = pd.DataFrame({
        "text": [
            "Normal text",
            123,  # æ•°å­—
            True,  # å¸ƒå°”å€¼
            None,  # ç©ºå€¼
            ["list", "of", "items"]  # åˆ—è¡¨
        ]
    })
    
    # åº”è¯¥èƒ½å¤„ç†æ‰€æœ‰ç±»å‹ï¼Œå°†å®ƒä»¬è½¬æ¢ä¸ºå­—ç¬¦ä¸²
    result = processor.process_tokens(mixed_data["text"])
    assert len(result) == len(mixed_data)

def test_token_processing_concurrent_processing(model_path):
    """æµ‹è¯•å¹¶å‘å¤„ç†ã€‚"""
    processor = TokenProcessing(model_path)
    large_df = pd.DataFrame({
        "text": ["Hello world! " * 100] * 100
    })
    
    # ä½¿ç”¨å¤šä¸ªçº¿ç¨‹å¤„ç†
    result1 = processor.process_tokens(large_df["text"])
    result2 = processor.process_tokens(large_df["text"])
    
    # ç»“æœåº”è¯¥ç›¸åŒ
    pd.testing.assert_frame_equal(result1, result2)

def test_token_processing_visualization(model_path, tmp_path):
    """æµ‹è¯•tokenåˆ†å¸ƒå¯è§†åŒ–åŠŸèƒ½"""
    processor = TokenProcessing(model_path)
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    df = pd.DataFrame({
        'input_tokens': [[1, 2, 3], [2, 3, 4], [3, 4, 5]]
    })
    
    # æµ‹è¯•ä¿å­˜åˆ°ä¸´æ—¶ç›®å½•
    save_path = os.path.join(tmp_path, "distribution.png")
    distribution = processor.compute_distribution(df, save_path)
    
    # éªŒè¯ç»“æœ
    assert os.path.exists(save_path)
    assert isinstance(distribution, dict)
    assert len(distribution) > 0
    
def test_token_processing_empty_visualization(model_path, tmp_path):
    """æµ‹è¯•ç©ºæ•°æ®çš„å¯è§†åŒ–å¤„ç†"""
    processor = TokenProcessing(model_path)
    
    # å‡†å¤‡ç©ºæ•°æ®
    df = pd.DataFrame({
        'input_tokens': []
    })
    
    # æµ‹è¯•ä¿å­˜åˆ°ä¸´æ—¶ç›®å½•
    save_path = os.path.join(tmp_path, "empty_distribution.png")
    distribution = processor.compute_distribution(df, save_path)
    
    # éªŒè¯ç»“æœ
    assert isinstance(distribution, dict)
    assert len(distribution) == 0
    
def test_token_processing_invalid_tokens(model_path):
    """æµ‹è¯•æ— æ•ˆtokenæ•°æ®çš„å¤„ç†"""
    processor = TokenProcessing(model_path)
    
    # å‡†å¤‡åŒ…å«Noneå’Œéåˆ—è¡¨æ•°æ®çš„DataFrame
    df = pd.DataFrame({
        'input_tokens': [None, 123, [1, 2, 3], "invalid"]
    })
    
    # æµ‹è¯•åˆ†å¸ƒè®¡ç®—
    distribution = processor.compute_distribution(df)
    
    # éªŒè¯ç»“æœ
    assert isinstance(distribution, dict)
    assert len(distribution) > 0  # åº”è¯¥åªåŒ…å«æœ‰æ•ˆçš„token
    
def test_token_processing_write_permission(model_path, tmp_path):
    """æµ‹è¯•å†™å…¥æƒé™æ£€æŸ¥"""
    processor = TokenProcessing(model_path)
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    df = pd.DataFrame({
        'input_tokens': [[1, 2, 3]]
    })
    
    # åˆ›å»ºä¸€ä¸ªåªè¯»ç›®å½•
    readonly_dir = os.path.join(tmp_path, "readonly")
    os.makedirs(readonly_dir)
    save_path = os.path.join(readonly_dir, "distribution.png")
    
    # åˆ›å»ºä¸€ä¸ªç©ºæ–‡ä»¶å¹¶è®¾ç½®ä¸ºåªè¯»
    with open(save_path, 'w') as f:
        f.write('')
    
    # åœ¨Windowsä¸Šè®¾ç½®æ–‡ä»¶å’Œç›®å½•ä¸ºåªè¯»
    if os.name == 'nt':
        os.system(f'attrib +r "{save_path}"')
        os.system(f'attrib +r "{readonly_dir}"')
    else:
        os.chmod(readonly_dir, 0o444)
        os.chmod(save_path, 0o444)
    
    # æµ‹è¯•å†™å…¥æƒé™æ£€æŸ¥
    with pytest.raises(ValueError, match="æ²¡æœ‰å†™å…¥æƒé™"):
        processor.compute_distribution(df, save_path)
        
def test_token_processing_mixed_columns(model_path):
    """æµ‹è¯•åŒæ—¶åŒ…å«input_tokenså’Œtokenåˆ—çš„æƒ…å†µ"""
    processor = TokenProcessing(model_path)
    
    # å‡†å¤‡åŒ…å«ä¸¤ç§åˆ—çš„æ•°æ®
    df = pd.DataFrame({
        'input_tokens': [[1, 2, 3], [4, 5, 6]],
        'token': ['a', 'b']
    })
    
    # æµ‹è¯•åˆ†å¸ƒè®¡ç®—ï¼ˆåº”è¯¥ä¼˜å…ˆä½¿ç”¨input_tokensï¼‰
    distribution = processor.compute_distribution(df)
    
    # éªŒè¯ç»“æœ
    assert isinstance(distribution, dict)
    assert len(distribution) > 0
    assert 1 in [float(k) for k in distribution.keys()]  # ç¡®è®¤ä½¿ç”¨äº†input_tokensåˆ—
    
def test_token_processing_visualization_error(model_path, tmp_path):
    """æµ‹è¯•å¯è§†åŒ–è¿‡ç¨‹ä¸­çš„é”™è¯¯å¤„ç†"""
    processor = TokenProcessing(model_path)
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    df = pd.DataFrame({
        'input_tokens': [[1, 2, 3]]
    })
    
    # ä½¿ç”¨æ— æ•ˆçš„æ–‡ä»¶åï¼ˆåŒ…å«Windowsä¸Šçš„éæ³•å­—ç¬¦ï¼‰
    save_path = os.path.join(tmp_path, "test<>:.png")
    
    # æµ‹è¯•é”™è¯¯å¤„ç†
    with pytest.raises(ValueError, match="åŒ…å«æ— æ•ˆå­—ç¬¦"):
        processor.compute_distribution(df, save_path)

@pytest.fixture
def mock_dataframe():
    """åˆ›å»ºæ¨¡æ‹ŸDataFrameã€‚"""
    return pd.DataFrame({
        "text": ["Hello", "Hi", "Hey"],
        "input_tokens": [[1, 2, 3], [4, 5], [6, 7, 8]],
        "decoded_text": ["Hello", "Hi", "Hey"]
    }) 